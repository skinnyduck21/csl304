{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuFRrg55Mr_d",
        "outputId": "7648db48-7a5d-43f5-d749-0dd98f4ab35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 2 iterations\n",
            "\n",
            "Optimal Policy π*:\n",
            "  S1: a2\n",
            "  S2: a2\n",
            "  S3: a2\n",
            "  S4: a1\n",
            "\n",
            "Optimal Value Function V*:\n",
            "  S1: -4.934\n",
            "  S2: -5.258\n",
            "  S3: -3.800\n",
            "  S4: -2.000\n",
            "  G: 0.000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ================================\n",
        "# POLICY ITERATION for the given MDP\n",
        "# ================================\n",
        "\n",
        "# States: S1, S2, S3, S4, G\n",
        "states = ['S1', 'S2', 'S3', 'S4', 'G']\n",
        "n_states = len(states)\n",
        "\n",
        "# Actions: a1, a2\n",
        "actions = ['a1', 'a2']\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Transition probabilities T[s][a] = [(next_state, prob)]\n",
        "T = {\n",
        "    'S1': {\n",
        "        'a1': [('S2', 0.8), ('S3', 0.2)],\n",
        "        'a2': [('S3', 0.7), ('S4', 0.3)]\n",
        "    },\n",
        "    'S2': {\n",
        "        'a1': [('S1', 0.5), ('S3', 0.4), ('G', 0.1)],\n",
        "        'a2': [('S3', 0.9), ('S4', 0.1)]\n",
        "    },\n",
        "    'S3': {\n",
        "        'a1': [('S2', 0.6), ('G', 0.4)],\n",
        "        'a2': [('S4', 1.0)]\n",
        "    },\n",
        "    'S4': {\n",
        "        'a1': [('G', 1.0)]\n",
        "    },\n",
        "    'G': {}\n",
        "}\n",
        "\n",
        "# Reward (or cost): -2 per step (since cost = 2)\n",
        "reward = -2\n",
        "\n",
        "# Initialize policy arbitrarily: π(s) = a1 for all non-goal states\n",
        "policy = {s: 'a1' for s in states if s != 'G'}\n",
        "\n",
        "def policy_evaluation(policy, V, theta=1e-6):\n",
        "    \"\"\"Evaluate a given policy until convergence.\"\"\"\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            if s == 'G':\n",
        "                continue\n",
        "            v = V[s]\n",
        "            a = policy[s]\n",
        "            V[s] = 0\n",
        "            for s_next, p in T[s][a]:\n",
        "                r = reward\n",
        "                V[s] += p * (r + gamma * V[s_next])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, policy):\n",
        "    \"\"\"Improve the policy based on current value function.\"\"\"\n",
        "    policy_stable = True\n",
        "    for s in states:\n",
        "        if s == 'G':\n",
        "            continue\n",
        "        old_action = policy[s]\n",
        "        action_values = {}\n",
        "        for a in T[s].keys():\n",
        "            val = 0\n",
        "            for s_next, p in T[s][a]:\n",
        "                r = reward\n",
        "                val += p * (r + gamma * V[s_next])\n",
        "            action_values[a] = val\n",
        "        # choose action with maximum value (since rewards are negative, this means minimum cost)\n",
        "        best_action = max(action_values, key=action_values.get)\n",
        "        policy[s] = best_action\n",
        "        if old_action != best_action:\n",
        "            policy_stable = False\n",
        "    return policy, policy_stable\n",
        "\n",
        "# ================================\n",
        "# Main Policy Iteration Loop\n",
        "# ================================\n",
        "V = {s: 0 for s in states}\n",
        "iteration = 0\n",
        "\n",
        "while True:\n",
        "    iteration += 1\n",
        "    V = policy_evaluation(policy, V)\n",
        "    policy, stable = policy_improvement(V, policy)\n",
        "    if stable:\n",
        "        break\n",
        "\n",
        "# ================================\n",
        "# Results\n",
        "# ================================\n",
        "print(\"Converged after\", iteration, \"iterations\")\n",
        "print(\"\\nOptimal Policy π*:\")\n",
        "for s in policy:\n",
        "    print(f\"  {s}: {policy[s]}\")\n",
        "\n",
        "print(\"\\nOptimal Value Function V*:\")\n",
        "for s in V:\n",
        "    print(f\"  {s}: {V[s]:.3f}\")\n"
      ]
    }
  ]
}